use std::cell::RefCell;
use std::collections::HashMap;
use std::collections::VecDeque;
use std::fs;
use std::ops::Range;
use std::path::PathBuf;

use cake_lex::DFAScanner;
use cake_lex::LexemeSet;
use lexeme_sets::c_lexemes::CLexemes;
use lexeme_sets::c_preprocessor::CPreprocessor;

// autogenerated lexeme sets
pub mod lexeme_sets;

// TODO: think about what interface should be for backtracking due to
// parsing troubles
pub trait TokenStream<T: LexemeSet> {
    fn eat(&mut self, lexeme: T) -> bool;
    fn peek(&mut self) -> Option<(T, &str, usize)>;
    fn peekn(&mut self, n: usize) -> Option<(T, &str, usize)>;
    fn advance(&mut self) -> Option<(T, &str, usize)>;

    fn rollback(&mut self, target: usize);
    fn get_location(&self) -> usize;
}

// TODO: processed token stream to implement preprocessor
// don't think you have to do a separate pass for preprocessing
const BUFFER_SIZE: usize = 10;
pub struct RawTokenStream<'a, T>
where
    T: LexemeSet,
{
    cursor: usize,
    scanner: DFAScanner,
    source: &'a [u8],

    buffer: VecDeque<(T, &'a str, usize)>,
}

impl<'a, T: LexemeSet> RawTokenStream<'a, T> {
    pub fn new(scanner: DFAScanner, source: &'a [u8]) -> RawTokenStream<'a, T> {
        let retval = RawTokenStream {
            cursor: 0,
            scanner,
            source,

            buffer: VecDeque::new(),
        };

        retval
    }

    fn refill_buffer(&mut self) {
        self.refill_buffer_to_size(BUFFER_SIZE);
    }

    fn refill_buffer_to_size(&mut self, size: usize) {
        while self.buffer.len() < size {
            let (lexeme, action, next_cursor) = self.scanner.next_word(self.source, self.cursor);

            if action == -1 {
                break;
            }
            // if action as u32 == CLexemes::Whitespace.to_id() {
            //     self.cursor = next_cursor;
            //     continue;
            // }
            let l = T::from_id(action as u32).expect("invalid action (is the scanner compatible?)");
            self.buffer.push_back((l, lexeme, self.cursor));
            self.cursor = next_cursor;
        }
    }
}

// TODO: optimize this
impl<'a, T> TokenStream<T> for RawTokenStream<'a, T>
where
    T: LexemeSet,
{
    fn eat(&mut self, expected_lexeme: T) -> bool {
        self.refill_buffer();
        let matched = match self.buffer.pop_front() {
            Some((lexeme, _, _)) if lexeme == expected_lexeme => true,
            _ => false,
        };

        matched
    }

    fn peek(&mut self) -> Option<(T, &str, usize)> {
        self.refill_buffer();
        self.buffer.front().copied()
    }

    fn advance(&mut self) -> Option<(T, &str, usize)> {
        self.refill_buffer();
        let old_memo = self.buffer.pop_front();
        old_memo
    }

    fn rollback(&mut self, target: usize) {
        self.cursor = target;
        self.buffer.clear();
    }

    fn get_location(&self) -> usize {
        self.cursor
    }

    fn peekn(&mut self, n: usize) -> Option<(T, &str, usize)> {
        self.refill_buffer_to_size(n + 1);
        self.buffer.get(n).copied()
    }
}

// specialized implementation for C lexemes
// implements preprocessing logic
struct Preprocessor {
    preprocess_scanner: DFAScanner,
    main_scanner: DFAScanner,

    sources_map: HashMap<PathBuf, SourceFileDescriptor>,
    sources: Vec<Box<str>>, // Box<str> is preferable, since no need to mutate source files (?)

    cursor_stack: Vec<SourceCursor>,
    macros: HashMap<String, PreprocessorMacro>,

    conditional_stack: Vec<ConditionalState>,
    pp_token_line_buffer: VecDeque<PreprocessorToken>,
    clexeme_buffer: VecDeque<CToken>,
}

// invariant: span always refers to current source
// should be upheld as long as we only get_line once pp_token_buffer is empty
#[derive(Clone, Copy)]
struct PreprocessorToken {
    token: CPreprocessor,
    span: (usize, usize),
}

struct CToken {
    token: CLexemes,
    span: (usize, usize),
}

#[derive(Clone, Copy, Debug)]
enum ConditionalState {
    NoneTaken,
    Active,
    SomeTaken,
}

struct FunctionMacroInvocation {}

struct SourceFileDescriptor {
    // header guard optimization - #pragma once
    header_guard: bool,
    source_idx: usize,
}

#[derive(Debug, Clone)]
struct SourceCursor {
    filepath: PathBuf,
    file_idx: usize,
    cursor: usize,
    line: usize,
}

enum PreprocessorMacro {
    ObjectMacro,
    FunctionMacro,
}

impl Preprocessor {
    pub fn new(file: PathBuf, contents: String) -> Self {
        let preprocess_scanner = DFAScanner::new(CPreprocessor::load_table());
        let main_scanner = DFAScanner::new(CLexemes::load_table());

        let sources = vec![contents.into_boxed_str()];
        let mut sources_map = HashMap::new();
        sources_map.insert(
            file.clone(),
            SourceFileDescriptor {
                header_guard: false,
                source_idx: 0,
            },
        );

        Self {
            preprocess_scanner,
            main_scanner,
            sources_map,
            sources,
            cursor_stack: vec![SourceCursor {
                filepath: file,
                file_idx: 0,
                cursor: 0,
                line: 1,
            }],

            macros: HashMap::new(),
            pp_token_line_buffer: VecDeque::new(),
            clexeme_buffer: VecDeque::new(),
            conditional_stack: Vec::new(),
        }
    }

    fn include_file(&mut self, path: PathBuf) {
        // 1. check for include guard / if file is already open
        if let Some(src_file) = self.sources_map.get(&path) {
            if src_file.header_guard {
                return;
            } else {
                self.cursor_stack.push(SourceCursor {
                    filepath: path,
                    file_idx: src_file.source_idx,
                    cursor: 0,
                    line: 1,
                });

                return;
            }
        }

        // 2. read file
        let contents = fs::read_to_string(&path).expect("err while opening file");

        // 3. update cursor stack and opened src file map
        let idx = self.sources.len();
        self.sources.push(contents.into_boxed_str());
        let src_descriptor = SourceFileDescriptor {
            header_guard: false,
            source_idx: idx,
        };
        self.sources_map.insert(path.clone(), src_descriptor);
        self.cursor_stack.push(SourceCursor {
            filepath: path,
            file_idx: idx,
            cursor: 0,
            line: 1,
        });
    }

    fn get_current_src_str(&self) -> &str {
        self.sources[self.current_cursor().file_idx].as_ref()
    }

    fn get_remaining_src_str(&self) -> &str {
        let current_src_str = self.get_current_src_str();
        &current_src_str[self.current_cursor().cursor..]
    }

    // invariant: cursor stack must never be fully empty until compiler is finished
    fn current_cursor(&self) -> &SourceCursor {
        self.cursor_stack.last().unwrap()
    }

    fn current_cursor_mut(&mut self) -> &mut SourceCursor {
        self.cursor_stack.last_mut().unwrap()
    }

    fn get_text(&self, span: (usize, usize)) -> &str {
        &self.get_current_src_str()[span.0..span.1]
    }

    // line-by-line processing could lead to super pathological cases (e.g. gigantic single line macros)
    // or if someone decides to put their entire source file in one big line
    // but this is much simpler
    fn process_line(&mut self) -> bool {
        // if current file is empty, pop cursor stack
        let mut remaining_src_str = self.get_remaining_src_str();
        while remaining_src_str.is_empty() {
            match self.cursor_stack.pop() {
                Some(src_cursor) => {
                    if self.cursor_stack.is_empty() {
                        self.cursor_stack.push(src_cursor);
                        return false;
                    } else {
                        remaining_src_str = self.get_remaining_src_str();
                    }
                }
                None => unreachable!("cursor stack should always be nonempty"),
            }
        }

        let mut prev_char: char = '\n';
        let mut physical_lines = 0;
        let logical_line_break = remaining_src_str
            .find(|c| {
                if prev_char != '\\' && c == '\n' {
                    physical_lines += 1;
                    true
                } else {
                    physical_lines += if c == '\n' { 1 } else { 0 };
                    prev_char = c;
                    false
                }
            })
            // include newline within line (if this line doesn't include final line of file)
            .map(|p| p + '\n'.len_utf8());

        let old_cursor = self.current_cursor().cursor;
        if let Some(line_break) = logical_line_break {
            self.current_cursor_mut().line += physical_lines;
            self.current_cursor_mut().cursor = line_break;
        } else {
            self.current_cursor_mut().line += physical_lines;
            self.current_cursor_mut().cursor = self.get_current_src_str().len();
        }

        let mut cursor = old_cursor;
        loop {
            let src_str = self.get_current_src_str();
            let (_, action, next_cursor) = self
                .preprocess_scanner
                .next_word(src_str.as_bytes(), cursor);

            if action == -1 {
                break;
            }

            let token = CPreprocessor::from_id(action as u32)
                .expect("C preprocessor DFA should be infallible");

            if let CPreprocessor::Newline = token {
                break;
            }
            if let CPreprocessor::Splice = token {
                continue;
            }

            let preprocessor_token = PreprocessorToken {
                token,
                span: (cursor, next_cursor),
            };

            cursor = next_cursor;
            self.pp_token_line_buffer.push_back(preprocessor_token);
        }
        return true;
    }

    fn preprocessor_directive(&mut self) {}

    // precondition: pp_token_line_buffer contains all preprocessing tokens (excluding splices and final newline) from a single logical line
    fn convert_line_to_clexemes(&mut self) {
        // 1. check for preprocessing directive
        let non_whitespace = self.pp_token_line_buffer.iter().filter_map(|t| {
            if t.token != CPreprocessor::Whitespace {
                Some(t.token)
            } else {
                None
            }
        });

        if non_whitespace
            .take(2)
            .eq([CPreprocessor::Hash, CPreprocessor::Identifier])
        {
            self.preprocessor_directive();
        }
    }
}

impl TokenStream<CLexemes> for Preprocessor {
    fn eat(&mut self, lexeme: CLexemes) -> bool {
        todo!()
    }

    fn peek(&mut self) -> Option<(CLexemes, &str, usize)> {
        todo!()
    }

    fn peekn(&mut self, n: usize) -> Option<(CLexemes, &str, usize)> {
        todo!()
    }

    fn advance(&mut self) -> Option<(CLexemes, &str, usize)> {
        todo!()
    }

    fn rollback(&mut self, target: usize) {
        todo!()
    }

    fn get_location(&self) -> usize {
        todo!()
    }
}
